{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for Bengali Medical Chatbot\n",
    "\n",
    "This notebook explores the available datasets for training the Bengali medical chatbot.\n",
    "\n",
    "## Datasets:\n",
    "1. Kaggle AI Medical Chatbot Dataset\n",
    "2. Medical Chatbot Dataset (Sarfaraz)\n",
    "3. BanglaHealth Paraphrase Dataset\n",
    "4. Medical Terms Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.preprocessor import MedicalDataPreprocessor\n",
    "from utils.logger import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "data_dir = Path('../data/raw')\n",
    "\n",
    "# Load datasets\n",
    "datasets = {}\n",
    "\n",
    "# Kaggle Medical Dataset 1\n",
    "try:\n",
    "    datasets['kaggle_medical_1'] = pd.read_csv(data_dir / 'ai_medical_chatbot.csv')\n",
    "    print(f\"✓ Loaded Kaggle Medical Dataset 1: {datasets['kaggle_medical_1'].shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Kaggle Medical Dataset 1 not found\")\n",
    "\n",
    "# Kaggle Medical Dataset 2\n",
    "try:\n",
    "    datasets['kaggle_medical_2'] = pd.read_csv(data_dir / 'medical_chatbot_dataset.csv')\n",
    "    print(f\"✓ Loaded Kaggle Medical Dataset 2: {datasets['kaggle_medical_2'].shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Kaggle Medical Dataset 2 not found\")\n",
    "\n",
    "# BanglaHealth Dataset\n",
    "try:\n",
    "    datasets['bangla_health'] = pd.read_json(data_dir / 'bangla_health_paraphrases.json')\n",
    "    print(f\"✓ Loaded BanglaHealth Dataset: {datasets['bangla_health'].shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ BanglaHealth Dataset not found\")\n",
    "\n",
    "# Medical Terms Dictionary\n",
    "try:\n",
    "    with open(data_dir / 'medical_terms_en_bn.json', 'r', encoding='utf-8') as f:\n",
    "        medical_terms = json.load(f)\n",
    "    print(f\"✓ Loaded Medical Terms Dictionary: {len(medical_terms)} terms\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Medical Terms Dictionary not found\")\n",
    "    medical_terms = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of all datasets\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"  Sample data:\")\n",
    "    for col in df.columns[:2]:  # Show first 2 columns\n",
    "        sample_value = str(df[col].iloc[0])[:100] + \"...\" if len(str(df[col].iloc[0])) > 100 else str(df[col].iloc[0])\n",
    "        print(f\"    {col}: {sample_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor for quality analysis\n",
    "preprocessor = MedicalDataPreprocessor()\n",
    "\n",
    "def analyze_data_quality(df, name):\n",
    "    \"\"\"Analyze data quality for a dataset.\"\"\"\n",
    "    print(f\"\\nData Quality Analysis - {name.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(\"Missing values:\")\n",
    "        for col, count in missing[missing > 0].items():\n",
    "            print(f\"  {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"✓ No missing values\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Duplicate rows: {duplicates} ({duplicates/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Text columns analysis\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            lengths = df[col].astype(str).str.len()\n",
    "            print(f\"\\n{col} statistics:\")\n",
    "            print(f\"  Length - Mean: {lengths.mean():.1f}, Median: {lengths.median():.1f}\")\n",
    "            print(f\"  Length - Min: {lengths.min()}, Max: {lengths.max()}\")\n",
    "            print(f\"  Unique values: {df[col].nunique()} ({df[col].nunique()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Analyze each dataset\n",
    "for name, df in datasets.items():\n",
    "    analyze_data_quality(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Language Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_languages_in_dataset(df, text_columns):\n",
    "    \"\"\"Detect languages in text columns.\"\"\"\n",
    "    language_stats = {}\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            languages = df[col].astype(str).apply(preprocessor.detect_language)\n",
    "            language_stats[col] = languages.value_counts()\n",
    "    \n",
    "    return language_stats\n",
    "\n",
    "# Analyze languages in each dataset\n",
    "print(\"Language Distribution Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    \n",
    "    # Get text columns\n",
    "    text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if text_cols:\n",
    "        lang_stats = detect_languages_in_dataset(df, text_cols[:2])  # Analyze first 2 text columns\n",
    "        \n",
    "        for col, stats in lang_stats.items():\n",
    "            print(f\"  {col}:\")\n",
    "            for lang, count in stats.items():\n",
    "                print(f\"    {lang}: {count} ({count/stats.sum()*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  No text columns found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Medical Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_medical_content(df, text_columns, medical_terms):\n",
    "    \"\"\"Analyze medical content in text columns.\"\"\"\n",
    "    medical_stats = {}\n",
    "    \n",
    "    # Create list of all medical terms (English and Bengali)\n",
    "    all_terms = list(medical_terms.keys()) + list(medical_terms.values())\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            # Count medical terms in each text\n",
    "            medical_counts = []\n",
    "            for text in df[col].astype(str):\n",
    "                text_lower = text.lower()\n",
    "                count = sum(1 for term in all_terms if term.lower() in text_lower)\n",
    "                medical_counts.append(count)\n",
    "            \n",
    "            medical_stats[col] = {\n",
    "                'mean_terms': np.mean(medical_counts),\n",
    "                'median_terms': np.median(medical_counts),\n",
    "                'max_terms': np.max(medical_counts),\n",
    "                'texts_with_medical': sum(1 for count in medical_counts if count > 0),\n",
    "                'medical_percentage': sum(1 for count in medical_counts if count > 0) / len(medical_counts) * 100\n",
    "            }\n",
    "    \n",
    "    return medical_stats\n",
    "\n",
    "# Analyze medical content\n",
    "print(\"Medical Content Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if text_cols and medical_terms:\n",
    "        med_stats = analyze_medical_content(df, text_cols[:2], medical_terms)\n",
    "        \n",
    "        for col, stats in med_stats.items():\n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    Avg medical terms per text: {stats['mean_terms']:.2f}\")\n",
    "            print(f\"    Texts with medical content: {stats['texts_with_medical']} ({stats['medical_percentage']:.1f}%)\")\n",
    "            print(f\"    Max medical terms in single text: {stats['max_terms']}\")\n",
    "    else:\n",
    "        print(\"  No text columns or medical terms dictionary available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Dataset Analysis Visualizations', fontsize=16)\n",
    "\n",
    "# 1. Dataset sizes\n",
    "if datasets:\n",
    "    dataset_sizes = {name: len(df) for name, df in datasets.items()}\n",
    "    axes[0, 0].bar(dataset_sizes.keys(), dataset_sizes.values())\n",
    "    axes[0, 0].set_title('Dataset Sizes')\n",
    "    axes[0, 0].set_ylabel('Number of Records')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Text length distribution (for first dataset with text)\n",
    "if datasets:\n",
    "    first_dataset = list(datasets.values())[0]\n",
    "    text_col = first_dataset.select_dtypes(include=['object']).columns[0]\n",
    "    lengths = first_dataset[text_col].astype(str).str.len()\n",
    "    axes[0, 1].hist(lengths, bins=50, alpha=0.7)\n",
    "    axes[0, 1].set_title(f'Text Length Distribution ({text_col})')\n",
    "    axes[0, 1].set_xlabel('Text Length')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Medical terms frequency\n",
    "if medical_terms:\n",
    "    # Show top 10 most common medical terms\n",
    "    term_lengths = {term: len(term) for term in medical_terms.keys()}\n",
    "    sorted_terms = sorted(term_lengths.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    terms, lengths = zip(*sorted_terms)\n",
    "    axes[1, 0].barh(terms, lengths)\n",
    "    axes[1, 0].set_title('Medical Terms Length (Top 10)')\n",
    "    axes[1, 0].set_xlabel('Term Length')\n",
    "\n",
    "# 4. Dataset column counts\n",
    "if datasets:\n",
    "    column_counts = {name: len(df.columns) for name, df in datasets.items()}\n",
    "    axes[1, 1].bar(column_counts.keys(), column_counts.values())\n",
    "    axes[1, 1].set_title('Number of Columns per Dataset')\n",
    "    axes[1, 1].set_ylabel('Number of Columns')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data from each dataset\n",
    "print(\"Sample Data Inspection:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()} - Sample Records:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Show first 3 records\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"\\nRecord {i+1}:\")\n",
    "        for col in df.columns:\n",
    "            value = str(df.iloc[i][col])\n",
    "            # Truncate long values\n",
    "            if len(value) > 200:\n",
    "                value = value[:200] + \"...\"\n",
    "            print(f\"  {col}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Preparation Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Preparation Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check for missing values\n",
    "for name, df in datasets.items():\n",
    "    missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    if missing_pct > 5:\n",
    "        recommendations.append(f\"⚠️  {name}: High missing values ({missing_pct:.1f}%) - implement imputation strategy\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    dup_pct = (df.duplicated().sum() / len(df)) * 100\n",
    "    if dup_pct > 10:\n",
    "        recommendations.append(f\"⚠️  {name}: High duplicate rate ({dup_pct:.1f}%) - implement deduplication\")\n",
    "    \n",
    "    # Check text length distribution\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        lengths = df[col].astype(str).str.len()\n",
    "        if lengths.max() > 2000:\n",
    "            recommendations.append(f\"⚠️  {name}.{col}: Very long texts (max: {lengths.max()}) - consider truncation\")\n",
    "        if lengths.min() < 10:\n",
    "            recommendations.append(f\"⚠️  {name}.{col}: Very short texts (min: {lengths.min()}) - consider filtering\")\n",
    "\n",
    "# General recommendations\n",
    "general_recommendations = [\n",
    "    \"✅ Implement comprehensive text cleaning pipeline\",\n",
    "    \"✅ Create stratified train/validation/test splits\",\n",
    "    \"✅ Implement quality filtering based on text length and medical content\",\n",
    "    \"✅ Set up translation pipeline for English datasets\",\n",
    "    \"✅ Implement cultural adaptation for Bengali medical terms\",\n",
    "    \"✅ Create data augmentation strategy for balanced training\",\n",
    "    \"✅ Implement medical accuracy validation with expert review\"\n",
    "]\n",
    "\n",
    "print(\"\\nSpecific Issues Found:\")\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "else:\n",
    "    print(\"✅ No major data quality issues detected\")\n",
    "\n",
    "print(\"\\nGeneral Recommendations:\")\n",
    "for rec in general_recommendations:\n",
    "    print(rec)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Data exploration complete! Ready for preprocessing phase.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics for export\n",
    "summary_stats = {\n",
    "    'datasets': {},\n",
    "    'medical_terms_count': len(medical_terms),\n",
    "    'total_records': sum(len(df) for df in datasets.values()),\n",
    "    'recommendations': recommendations + general_recommendations\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    summary_stats['datasets'][name] = {\n",
    "        'shape': df.shape,\n",
    "        'columns': list(df.columns),\n",
    "        'missing_values': df.isnull().sum().sum(),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'text_columns': text_cols,\n",
    "        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2\n",
    "    }\n",
    "    \n",
    "    if text_cols:\n",
    "        first_text_col = text_cols[0]\n",
    "        lengths = df[first_text_col].astype(str).str.len()\n",
    "        summary_stats['datasets'][name]['text_stats'] = {\n",
    "            'mean_length': lengths.mean(),\n",
    "            'median_length': lengths.median(),\n",
    "            'min_length': lengths.min(),\n",
    "            'max_length': lengths.max()\n",
    "        }\n",
    "\n",
    "# Save summary\n",
    "output_dir = Path('../experiments/results')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_dir / 'data_exploration_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_stats, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"✅ Summary statistics saved to: {output_dir / 'data_exploration_summary.json'}\")\n",
    "print(f\"\\nTotal datasets analyzed: {len(datasets)}\")\n",
    "print(f\"Total records across all datasets: {summary_stats['total_records']:,}\")\n",
    "print(f\"Medical terms in dictionary: {len(medical_terms)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
