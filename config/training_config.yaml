# Training Configuration for Bengali Medical Chatbot

training:
  # General training settings
  experiment_name: "bengali_medical_chatbot_v1"
  seed: 42
  deterministic: true
  
  # Training hyperparameters
  hyperparameters:
    batch_size: 8
    gradient_accumulation_steps: 4
    effective_batch_size: 32  # batch_size * gradient_accumulation_steps
    
    learning_rate: 5e-5
    weight_decay: 0.01
    warmup_ratio: 0.1
    max_grad_norm: 1.0
    
    num_epochs: 10
    max_steps: -1  # -1 means use num_epochs
    
    # Learning rate scheduling
    lr_scheduler: "cosine"
    lr_scheduler_kwargs:
      num_cycles: 0.5
      last_epoch: -1
  
  # Optimization
  optimizer:
    name: "AdamW"
    betas: [0.9, 0.999]
    eps: 1e-8
    
  # Mixed precision training
  mixed_precision:
    enabled: true
    fp16: true
    bf16: false  # Use if supported by hardware
  
  # Model checkpointing
  checkpointing:
    save_strategy: "steps"
    save_steps: 1000
    save_total_limit: 3
    load_best_model_at_end: true
    metric_for_best_model: "eval_loss"
    greater_is_better: false
  
  # Evaluation settings
  evaluation:
    evaluation_strategy: "steps"
    eval_steps: 500
    eval_accumulation_steps: 1
    
    # Metrics to track
    metrics:
      - "bleu"
      - "rouge"
      - "bert_score"
      - "medical_accuracy"
      - "cultural_appropriateness"
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 3
    threshold: 0.001
    
  # Logging and monitoring
  logging:
    logging_strategy: "steps"
    logging_steps: 100
    
    # Weights & Biases integration
    wandb:
      enabled: true
      project: "bengali-medical-chatbot"
      entity: "your-wandb-username"
      tags: ["bengali", "medical", "chatbot", "research"]
    
    # TensorBoard
    tensorboard:
      enabled: true
      log_dir: "experiments/logs/tensorboard"

  # Data loading
  dataloader:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2
    persistent_workers: true
    
  # Gradient checkpointing
  gradient_checkpointing: true
  
  # Model compilation (PyTorch 2.0+)
  compile_model: false  # Enable if using PyTorch 2.0+

# Multi-stage training
multi_stage:
  enabled: true
  
  stages:
    # Stage 1: General medical knowledge
    stage_1:
      name: "general_medical"
      epochs: 3
      learning_rate: 5e-5
      data_subset: "general_medical"
      
    # Stage 2: Bengali cultural adaptation
    stage_2:
      name: "cultural_adaptation"
      epochs: 3
      learning_rate: 3e-5
      data_subset: "cultural_adapted"
      
    # Stage 3: Specialized domains
    stage_3:
      name: "specialization"
      epochs: 4
      learning_rate: 1e-5
      data_subset: "specialized_domains"

# Distributed training
distributed:
  enabled: false
  backend: "nccl"
  world_size: 1
  rank: 0
  
# Hardware optimization
hardware:
  # GPU settings
  gpu:
    device_map: "auto"
    max_memory_per_gpu: "24GB"
    
  # CPU settings
  cpu:
    num_threads: 8
    
  # Memory optimization
  memory:
    gradient_checkpointing: true
    cpu_offload: false
    pin_memory: true

# Validation during training
validation:
  # Medical accuracy validation
  medical_validation:
    enabled: true
    expert_review_sample: 100
    automated_fact_check: true
    
  # Cultural appropriateness
  cultural_validation:
    enabled: true
    community_feedback: true
    sensitivity_check: true
    
  # Performance benchmarks
  benchmarks:
    response_time: 2.0  # seconds
    memory_usage: "8GB"
    throughput: 10  # requests per second

# Model export and deployment
export:
  formats:
    - "pytorch"
    - "onnx"
    - "tensorrt"  # if available
  
  optimization:
    quantization: true
    pruning: false
    distillation: false

# Experiment tracking
tracking:
  # Metrics to track
  metrics:
    - "train_loss"
    - "eval_loss"
    - "bleu_score"
    - "rouge_l"
    - "medical_accuracy"
    - "cultural_score"
    - "response_time"
    
  # Artifacts to save
  artifacts:
    - "model_checkpoints"
    - "training_logs"
    - "evaluation_results"
    - "sample_predictions"
